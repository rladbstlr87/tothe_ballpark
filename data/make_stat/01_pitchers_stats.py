import time
import pandas as pd
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC

driver = webdriver.Chrome()

headers = {
    "User-Agent": "Mozilla/5.0"
}

teams = ["LG", "HH", "LT", "SS", "SK", "NC", "OB", "HT", "KT", "WO"]
base_url = "https://www.koreabaseball.com/Record/Player/PitcherBasic/Basic1.aspx"
detail_url = "https://www.koreabaseball.com/Record/Player/PitcherDetail/Basic.aspx?playerId={}"

final_data = []

columns = [
    "team", "player_id", "player_name",
    "ERA", "G", "CG", "SHO", "W", "L", "SV", "HLD", "WPCT", "TBF", "NP", "IP", "H", "2B", "3B", "HR",
    "SAC", "SF", "BB", "IBB", "SO", "WP", "BK", "R", "ER", "BSV", "WHIP", "AVG", "QS"
]

driver.get(base_url)

for team in teams:
    print(f"\nğŸ“¦ íŒ€ ì„ íƒ ì¤‘: {team}")

    select_element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.ID, "cphContents_cphContents_cphContents_ddlTeam_ddlTeam"))
    )
    select = Select(select_element)
    select.select_by_value(team)
    time.sleep(2)

    def collect_player_infos():
        players = driver.find_elements(By.CSS_SELECTOR,
            "#cphContents_cphContents_cphContents_udpContent > div.record_result > table > tbody > tr > td:nth-child(2) > a"
        )
        return [
            (a.get_attribute("href").split("playerId=")[-1], a.text.strip())
            for a in players
        ]

    player_infos = collect_player_infos()

    try:
        next_btn = driver.find_element(By.ID, "cphContents_cphContents_cphContents_ucPager_btnNo2")
        next_btn.click()
        time.sleep(2)

        player_infos += collect_player_infos()

        prev_btn = driver.find_element(By.ID, "cphContents_cphContents_cphContents_ucPager_btnNo1")
        prev_btn.click()
        time.sleep(2)
    except:
        print("â¡ï¸ 2í˜ì´ì§€ ì—†ìŒ")

    print(f"ğŸ” ì´ {len(player_infos)}ëª… ì„ ìˆ˜ ë°œê²¬")

    team_data = []

    for player_id, player_name in player_infos:
        url = detail_url.format(player_id)
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.content, "html.parser")

        # âœ… ì²« ë²ˆì§¸ í…Œì´ë¸” (íŒ€ëª… ì œì™¸)
        try:
            table1 = soup.select_one("div.tbl-type02.tbl-type02-pd0.mb35 > table > tbody")
            data1 = [td.text.strip() for td in table1.select("td")][1:]
        except:
            data1 = []

        # âœ… ë‘ ë²ˆì§¸ í…Œì´ë¸”
        try:
            table2 = soup.select_one("div.player_records > div:nth-child(4) > table > tbody")
            data2 = [td.text.strip() for td in table2.select("td")]
        except:
            data2 = []

        if data1 and data2:
            row = [team, player_id, player_name] + data1 + data2
            team_data.append(row)
            print("âœ… ì €ì¥ë  ë°ì´í„°:", row)
        else:
            print(f"âš ï¸ ëˆ„ë½ë¨: {team} / {player_id} / {player_name}")

        time.sleep(0.3)

    final_data.extend(team_data)
    df_all = pd.DataFrame(final_data, columns=columns)
    df_all.to_csv("../all_pitcher_stats.csv", index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ ëˆ„ì  ì €ì¥ ì™„ë£Œ: all_pitcher_stats.csv")

driver.quit()
print("\nğŸ‰ ëª¨ë“  íˆ¬ìˆ˜ ë°ì´í„° ì €ì¥ ì™„ë£Œ!")

# âœ… ì„ì‹œ ì„ ìˆ˜ ì¶”ê°€
dummy_row = [
    "TMP", "1", "ì„ì‹œì„ ìˆ˜",
    "3.21", 10, 0, 0, 2, 1, 1, 0, "0.667", 150, 1000, 55.1, 48, 5, 0, 3,
    2, 1, 12, 1, 35, 1, 0, 15, 12, 0, "1.25", "0.220", 2
]
final_data.append(dummy_row)

df_all = pd.DataFrame(final_data, columns=columns)
df_all.to_csv("../all_pitcher_stats.csv", index=False, encoding="utf-8-sig")
print("ğŸ¯ ì„ì‹œ ì„ ìˆ˜ í¬í•¨ ìµœì¢… ì €ì¥ ì™„ë£Œ: all_pitcher_stats.csv")